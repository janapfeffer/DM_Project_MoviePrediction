{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from math import sqrt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# machine learning\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after import: 7517\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>id</th>\n",
       "      <th>runtime</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>History</th>\n",
       "      <th>Western</th>\n",
       "      <th>Music</th>\n",
       "      <th>Family</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Drama</th>\n",
       "      <th>...</th>\n",
       "      <th>orig_ml</th>\n",
       "      <th>orig_es</th>\n",
       "      <th>orig_bn</th>\n",
       "      <th>orig_ab</th>\n",
       "      <th>orig_wo</th>\n",
       "      <th>orig_ca</th>\n",
       "      <th>orig_ru</th>\n",
       "      <th>orig_id</th>\n",
       "      <th>orig_is</th>\n",
       "      <th>director</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000000</td>\n",
       "      <td>862</td>\n",
       "      <td>81.0</td>\n",
       "      <td>5415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65000000</td>\n",
       "      <td>8844</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60000000</td>\n",
       "      <td>949</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58000000</td>\n",
       "      <td>710</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98000000</td>\n",
       "      <td>1408</td>\n",
       "      <td>119.0</td>\n",
       "      <td>137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     budget    id  runtime  vote_count  History  Western  Music  Family  \\\n",
       "0  30000000   862     81.0        5415      0.0      0.0    0.0     1.0   \n",
       "1  65000000  8844    104.0        2413      0.0      0.0    0.0     1.0   \n",
       "2  60000000   949    170.0        1886      0.0      0.0    0.0     0.0   \n",
       "3  58000000   710    130.0        1194      0.0      0.0    0.0     0.0   \n",
       "4  98000000  1408    119.0         137      0.0      0.0    0.0     0.0   \n",
       "\n",
       "   Comedy  Drama  ...  orig_ml  orig_es  orig_bn  orig_ab  orig_wo  orig_ca  \\\n",
       "0     1.0    0.0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1     0.0    0.0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2     0.0    1.0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3     0.0    0.0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4     0.0    0.0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   orig_ru  orig_id  orig_is  director  \n",
       "0      0.0      0.0      0.0      1709  \n",
       "1      0.0      0.0      0.0      1645  \n",
       "2      0.0      0.0      0.0      2338  \n",
       "3      0.0      0.0      0.0      2231  \n",
       "4      0.0      0.0      0.0      2812  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "\n",
    "df_movies = pd.read_csv(\"regressionPreprocessing.csv\")\n",
    "print(\"Length after import: \" + str(len(df_movies)))\n",
    "df_movies = df_movies.fillna(0)\n",
    "df_movies.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we loaded all the libaries and data that we need, we can start with the classification tasks.\n",
    "\n",
    "Since we have a lot of features it makes sense to eliminate features that do not have a significant impact on the prediction to improve our perfromance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     budget  runtime  History  Western  Music  Family  Comedy  Drama  Foreign  \\\n",
      "0  30000000     81.0      0.0      0.0    0.0     1.0     1.0    0.0      0.0   \n",
      "1  65000000    104.0      0.0      0.0    0.0     1.0     0.0    0.0      0.0   \n",
      "2  60000000    170.0      0.0      0.0    0.0     0.0     0.0    1.0      0.0   \n",
      "3  58000000    130.0      0.0      0.0    0.0     0.0     0.0    0.0      0.0   \n",
      "4  98000000    119.0      0.0      0.0    0.0     0.0     0.0    0.0      0.0   \n",
      "\n",
      "   Action  ...  Thriller  TV Movie  Crime  Science Fiction  Fantasy  War  \\\n",
      "0     0.0  ...       0.0       0.0    0.0              0.0      0.0  0.0   \n",
      "1     0.0  ...       0.0       0.0    0.0              0.0      1.0  0.0   \n",
      "2     1.0  ...       1.0       0.0    1.0              0.0      0.0  0.0   \n",
      "3     1.0  ...       1.0       0.0    0.0              0.0      0.0  0.0   \n",
      "4     1.0  ...       0.0       0.0    0.0              0.0      0.0  0.0   \n",
      "\n",
      "   Adventure    rating  part_of_collection  director  \n",
      "0        0.0  3.598930                   1      1709  \n",
      "1        1.0  3.760163                   0      1645  \n",
      "2        0.0  3.905544                   0      2338  \n",
      "3        1.0  2.740334                   1      2231  \n",
      "4        1.0  3.710181                   0      2812  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['budget', 'runtime', 'History', 'Western', 'Music', 'Family', 'Comedy',\n",
       "       'Drama', 'Foreign', 'Action', 'Horror', 'Mystery', 'Romance',\n",
       "       'Animation', 'Documentary', 'Thriller', 'TV Movie', 'Crime',\n",
       "       'Science Fiction', 'Fantasy', 'War', 'Adventure', 'rating',\n",
       "       'part_of_collection', 'director'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns that are not needed maybe implement feature selction...\n",
    "\n",
    "# # Instantiate RFECV visualizer with a linear SVM classifier\n",
    "# model = LogisticRegression()\n",
    "# visualizer = RFECV(model, 3)\n",
    "\n",
    "# visualizer.fit(features_train, rating_train)        # Fit the data to the visualizer\n",
    "# visualizer.show()           # Finalize and render the figure\n",
    "\n",
    "# # Create the RFE object and compute a cross-validated score.\n",
    "# # The \"accuracy\" scoring is proportional to the number of correct\n",
    "# # classifications\n",
    "# # rfecv = RFECV(estimator=knn_reg, step=1, cv=stratified_10_fold_cv, scoring='accuracy')\n",
    "# # rfecv.fit(features_train, rating_train)\n",
    "\n",
    "# # print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# # # Plot number of features VS. cross-validation scores\n",
    "# # plt.figure()\n",
    "# # plt.xlabel(\"Number of features selected\")\n",
    "# # plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "# # plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "features_to_remove = ['vote_count','+18','id','actors','spokenLanguages','productionCountries','productionCompanies','original_language','hasHomepage','part_of_collection''orig_lv', 'orig_el', 'orig_fa', 'orig_tl', 'orig_ta', 'orig_th',\n",
    "       'orig_mn', 'orig_zh', 'orig_te', 'orig_kk', 'orig_zu', 'orig_et',\n",
    "       'orig_mr', 'orig_eu', 'orig_sv', 'orig_no', 'orig_pl', 'orig_cs',\n",
    "       'orig_cy', 'orig_bs', 'orig_de', 'orig_lo', 'orig_xx', 'orig_ko',\n",
    "       'orig_hu', 'orig_sr', 'orig_da', 'orig_pt', 'orig_nl', 'orig_en',\n",
    "       'orig_it', 'orig_tr', 'orig_hr', 'orig_cn', 'orig_ka', 'orig_ar',\n",
    "       'orig_ja', 'orig_0', 'orig_hi', 'orig_ro', 'orig_af', 'orig_sk',\n",
    "       'orig_fr', 'orig_fi', 'orig_he', 'orig_uk', 'orig_bg', 'orig_ml',\n",
    "       'orig_es', 'orig_bn', 'orig_ab', 'orig_wo', 'orig_ca', 'orig_ru',\n",
    "       'orig_id', 'orig_is', 'orig_lv', '18+']\n",
    "for i in features_to_remove:\n",
    "    if i in df_movies.columns:\n",
    "        df_movies = df_movies.drop(columns=i)\n",
    "print(df_movies.head(5))\n",
    "df_movies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have exactly the data we want we start with the spitting.\n",
    "\n",
    "1. First of all we separate the features from our targte(the rating).\n",
    "2. We split our data into training and test data in order to evaluate our model later. The proportions will be 60% to 40%\n",
    "3. We creaze a kfold cross validation that we will later use for our models in order to evaluate them better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4510 Features and 4510 Ratings\n",
      "Test: 3007 Features and 3007 Ratings\n"
     ]
    }
   ],
   "source": [
    "# separate features and target variable\n",
    "rating = df_movies['rating'] # weight\n",
    "features = df_movies.drop(columns=['rating'])\n",
    "\n",
    "# encode labels\n",
    "lab_enc = LabelEncoder()\n",
    "rating = lab_enc.fit_transform(rating)\n",
    "\n",
    "# create a train/test split\n",
    "features_train, features_test, rating_train, rating_test = train_test_split(\n",
    "    features, rating, test_size=0.4, random_state=42)\n",
    "\n",
    "print(\"Train: \" + str(len(features_train)) + \" Features and \" + str(len(rating_train)) + \" Ratings\")\n",
    "print(\"Test: \" + str(len(features_test)) + \" Features and \" + str(len(rating_test)) + \" Ratings\")\n",
    "\n",
    "# specify the cross validation\n",
    "stratified_10_fold_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial evaluation of different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Random Forest:\n",
      "Accuracy: 0.009311606252078483\n",
      "Precision: 0.00930982512706723\n",
      "Recall: 0.009311606252078483\n",
      "f1_score: 0.00875226437338114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores knn:\n",
      "Accuracy: 0.010309278350515464\n",
      "Precision: 0.0051446110623426635\n",
      "Recall: 0.010309278350515464\n",
      "f1_score: 0.006238776125480133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Decision Tree:\n",
      "Accuracy: 0.008313934153641503\n",
      "Precision: 0.008655934006509264\n",
      "Recall: 0.008313934153641503\n",
      "f1_score: 0.0081387459246287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Naive Bayes:\n",
      "Accuracy: 0.0029930162953109413\n",
      "Precision: 0.005010749114532075\n",
      "Recall: 0.0029930162953109413\n",
      "f1_score: 0.0016524254020128335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores SVC:\n",
      "Accuracy: 0.0019953441968739607\n",
      "Precision: 0.04099032698615306\n",
      "Recall: 0.0019953441968739607\n",
      "f1_score: 0.000822878262895502\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\d060445\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "##### create and fit a RandomForestClassifier\n",
    "rf_reg = RandomForestClassifier()\n",
    "rf_reg.fit(features_train, rating_train)\n",
    "rating_pred_rf = rf_reg.predict(features_test)\n",
    "\n",
    "print(\"Scores Random Forest:\")\n",
    "#compute the confusion matrix\n",
    "# cnf_matrix = confusion_matrix(rating_test, rating_pred_rf)\n",
    "# print(cnf_matrix)\n",
    "\n",
    "#compute accuracy score\n",
    "print(\"Accuracy: {}\".format(accuracy_score(rating_test, rating_pred_rf)))\n",
    "print(\"Precision: {}\".format(precision_score(rating_test, rating_pred_rf, average='weighted')))\n",
    "print(\"Recall: {}\".format(recall_score(rating_test, rating_pred_rf, average='weighted')))\n",
    "print(\"f1_score: {}\".format(f1_score(rating_test, rating_pred_rf, average='weighted')))\n",
    "\n",
    "##### create and fit a KNN\n",
    "knn_reg = KNeighborsClassifier()\n",
    "knn_reg.fit(features_train, rating_train)\n",
    "rating_pred_knn = knn_reg.predict(features_test)\n",
    "\n",
    "print(\"Scores knn:\")\n",
    "# #compute the confusion matrix\n",
    "# cnf_matrix = confusion_matrix(rating_test, rating_pred_knn)\n",
    "# print(cnf_matrix)\n",
    "\n",
    "#compute accuracy score\n",
    "print(\"Accuracy: {}\".format(accuracy_score(rating_test, rating_pred_knn)))\n",
    "print(\"Precision: {}\".format(precision_score(rating_test, rating_pred_knn, average='weighted')))\n",
    "print(\"Recall: {}\".format(recall_score(rating_test, rating_pred_knn, average='weighted')))\n",
    "print(\"f1_score: {}\".format(f1_score(rating_test, rating_pred_knn, average='weighted')))\n",
    "\n",
    "##### create and fit a DecisionTreeClassifier\n",
    "dt_reg = DecisionTreeClassifier()\n",
    "dt_reg.fit(features_train, rating_train)\n",
    "rating_pred_dt = dt_reg.predict(features_test)\n",
    "\n",
    "print(\"Scores Decision Tree:\")\n",
    "#compute the confusion matrix\n",
    "# cnf_matrix = confusion_matrix(rating_test, rating_pred_dt)\n",
    "# print(cnf_matrix)\n",
    "\n",
    "#compute accuracy score\n",
    "print(\"Accuracy: {}\".format(accuracy_score(rating_test, rating_pred_dt)))\n",
    "print(\"Precision: {}\".format(precision_score(rating_test, rating_pred_dt, average='weighted')))\n",
    "print(\"Recall: {}\".format(recall_score(rating_test, rating_pred_dt, average='weighted')))\n",
    "print(\"f1_score: {}\".format(f1_score(rating_test, rating_pred_dt, average='weighted')))\n",
    "\n",
    "##### create and fit a GaussianNB\n",
    "nb_reg = GaussianNB()\n",
    "nb_reg.fit(features_train, rating_train)\n",
    "rating_pred_nb = nb_reg.predict(features_test)\n",
    "\n",
    "print(\"Scores Naive Bayes:\")\n",
    "# #compute the confusion matrix\n",
    "# cnf_matrix = confusion_matrix(rating_test, rating_pred_nb)\n",
    "# print(cnf_matrix)\n",
    "\n",
    "#compute accuracy score\n",
    "print(\"Accuracy: {}\".format(accuracy_score(rating_test, rating_pred_nb)))\n",
    "print(\"Precision: {}\".format(precision_score(rating_test, rating_pred_nb, average='weighted')))\n",
    "print(\"Recall: {}\".format(recall_score(rating_test, rating_pred_nb, average='weighted')))\n",
    "print(\"f1_score: {}\".format(f1_score(rating_test, rating_pred_nb, average='weighted')))\n",
    "\n",
    "##### create and fit a SVC\n",
    "svc_reg = LinearSVC()\n",
    "svc_reg.fit(features_train, rating_train)\n",
    "rating_pred_svc = svc_reg.predict(features_test)\n",
    "\n",
    "print(\"Scores SVC:\")\n",
    "#compute the confusion matrix\n",
    "# cnf_matrix = confusion_matrix(rating_test, rating_pred_svc)\n",
    "# print(cnf_matrix)\n",
    "\n",
    "#compute accuracy score\n",
    "print(\"Accuracy: {}\".format(accuracy_score(rating_test, rating_pred_svc)))\n",
    "print(\"Precision: {}\".format(precision_score(rating_test, rating_pred_svc, average='weighted')))\n",
    "print(\"Recall: {}\".format(recall_score(rating_test, rating_pred_svc, average='weighted')))\n",
    "print(\"f1_score: {}\".format(f1_score(rating_test, rating_pred_svc, average='weighted')))\n",
    "\n",
    "#plot the confusion matrix\n",
    "#plot_confusion_matrix(cnf_matrix, classes=lab_enc.classes_, title='KNN Classifier')\n",
    "\n",
    "print()\n",
    "\n",
    "# metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create and fit a knn classifier\n",
    "# knn_reg = KNeighborsClassifier()\n",
    "\n",
    "# knn_reg.fit(features_train, rating_train)\n",
    "# rating_pred_knn = knn_reg.predict(features_test)\n",
    "\n",
    "# print(\"Scores knn:\")\n",
    "# #compute the confusion matrix\n",
    "# cnf_matrix = confusion_matrix(rating_test, rating_pred_knn)\n",
    "# print(cnf_matrix)\n",
    "\n",
    "\n",
    "# # #compute accuracy score\n",
    "\n",
    "# # accuracy_knn = cross_val_score(knn_reg, features_train, rating_train, cv=cross_val, scoring='accuracy')\n",
    "\n",
    "# # for i, acc in enumerate(accuracy_knn):\n",
    "# #     print(\"Fold {}: Accuracy = {}%\".format(i, acc * 100.0))\n",
    "\n",
    "# # print(\"Average Accuracy = {}%\".format(accuracy_knn.mean() * 100.0))\n",
    "# # # print(\"Accuracy: {}\".format(accuracy_score(rating_test, rating_pred_knn)))\n",
    "# # # print(\"Precision: {}\".format(precision_score(rating_test, rating_pred_knn, average='weighted')))\n",
    "# # # print(\"Recall: {}\".format(recall_score(rating_test, rating_pred_knn, average='weighted')))\n",
    "# # # print(\"f1_score: {}\".format(f1_score(rating_test, rating_pred_knn, average='weighted')))\n",
    "\n",
    "# # #Tuning of algorithm\n",
    "\n",
    "# # # specify the parameter grid\n",
    "# # parameters = {\n",
    "# #     'n_neighbors': range(2, 10)\n",
    "# # }\n",
    "\n",
    "\n",
    "# # # create the grid search instance\n",
    "# # grid_search_estimator = GridSearchCV(knn_reg, parameters, scoring='accuracy', cv=stratified_10_fold_cv, return_train_score=False)\n",
    "\n",
    "# # # run the grid search\n",
    "# # grid_search_estimator.fit(features_train, rating_train)\n",
    "\n",
    "# # # print the results of all hyper-parameter combinations\n",
    "# # results = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "# # #display(results)\n",
    "    \n",
    "# # # print the best parameter setting\n",
    "# # print(\"best score is {} with params {}\".format(grid_search_estimator.best_score_, grid_search_estimator.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### create and fit a RandomForestClassifier\n",
    "# rf_reg = RandomForestClassifier()\n",
    "# rf_reg.fit(features_train, rating_train)\n",
    "# rating_pred_rf = rf_reg.predict(features_test)\n",
    "\n",
    "# print(\"Scores knn:\")\n",
    "# #compute the confusion matrix\n",
    "# cnf_matrix = confusion_matrix(rating_test, rating_pred_rf)\n",
    "# print(cnf_matrix)\n",
    "\n",
    "# #compute accuracy score\n",
    "# print(\"Accuracy: {}\".format(accuracy_score(rating_test, rating_pred_rf)))\n",
    "# print(\"Precision: {}\".format(precision_score(rating_test, rating_pred_rf, average='weighted')))\n",
    "# print(\"Recall: {}\".format(recall_score(rating_test, rating_pred_rf, average='weighted')))\n",
    "# print(\"f1_score: {}\".format(f1_score(rating_test, rating_pred_rf, average='weighted')))\n",
    "\n",
    "# #plot the confusion matrix\n",
    "# #plot_confusion_matrix(cnf_matrix, classes=lab_enc.classes_, title='KNN Classifier')\n",
    "\n",
    "# #Tuning of algorithm\n",
    "\n",
    "# # specify the parameter grid\n",
    "# parameters = {\n",
    "#     'max_depth':[1,3,5,10]\n",
    "# }\n",
    "\n",
    "\n",
    "# # create the grid search instance\n",
    "# grid_search_estimator = GridSearchCV(rf_reg, parameters, scoring='accuracy', cv=stratified_10_fold_cv, return_train_score=False)\n",
    "\n",
    "# # run the grid search\n",
    "# grid_search_estimator.fit(features_train, rating_train)\n",
    "\n",
    "# # print the results of all hyper-parameter combinations\n",
    "# results = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "# #display(results)\n",
    "    \n",
    "# # print the best parameter setting\n",
    "# print(\"best score is {} with params {}\".format(grid_search_estimator.best_score_, grid_search_estimator.best_params_))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
